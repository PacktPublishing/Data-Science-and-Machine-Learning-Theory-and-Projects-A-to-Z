{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wSum(X,W):\n",
    "    h = torch.from_numpy(X)\n",
    "    z = torch.matmul(W,h)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(x):\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardStep(X,W_list):\n",
    "    h = torch.from_numpy(X)\n",
    "    for W in W_list:\n",
    "        z = torch.matmul(W,h)\n",
    "        h = activate(z)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParams(W_list,dW_list,lr):\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(W_list)):\n",
    "            W_list[i] -= lr*dW_list[i]\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_sgd(X,y,W_list,loss_fn,lr=0.0001,nepochs=100):\n",
    "    for epoch in range(nepochs):\n",
    "        avgLoss = []\n",
    "        for i in range(len(y)):\n",
    "            Xin = X[i,:]\n",
    "            yTrue = y[i]\n",
    "            y_hat = forwardStep(Xin,W_list)\n",
    "            loss = loss_fn(y_hat,torch.tensor(yTrue,dtype=torch.double))\n",
    "            loss.backward()\n",
    "            avgLoss.append(loss.item())\n",
    "            sys.stdout.flush()\n",
    "            dW_list = []\n",
    "            for j in range(len(W_list)):\n",
    "                dW_list.append(W_list[j].grad.data)\n",
    "            W_list = updateParams(W_list,dW_list,lr)\n",
    "            for j in range(len(W_list)):\n",
    "                W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,np.mean(np.array(avgLoss))))\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_batch(X,y,W_list,loss_fn,lr=0.0001,nepochs=100):\n",
    "    n = len(y)\n",
    "    for epoch in range(nepochs):\n",
    "        loss = 0\n",
    "        for i in range(n):\n",
    "            Xin = X[i,:]\n",
    "            yTrue = y[i]\n",
    "            y_hat = forwardStep(Xin,W_list)\n",
    "            loss += loss_fn(y_hat,torch.tensor(yTrue,dtype=torch.double))\n",
    "        loss = loss/n\n",
    "        loss.backward()\n",
    "        sys.stdout.flush()\n",
    "        dW_list = []\n",
    "        for j in range(len(W_list)):\n",
    "            dW_list.append(W_list[j].grad.data)\n",
    "        W_list = updateParams(W_list,dW_list,lr)\n",
    "        for j in range(len(W_list)):\n",
    "            W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,loss))\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_minibatch(X,y,W_list,loss_fn,lr=0.0001,nepochs=100,batchSize=16):\n",
    "    n = len(y)\n",
    "    numBatches = n//batchSize\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        for batch in range(numBatches):\n",
    "            X_batch = X[batch*batchSize:(batch+1)*batchSize,:]\n",
    "            y_batch = y[batch*batchSize:(batch+1)*batchSize]\n",
    "            loss = 0\n",
    "            for i in range(batchSize):\n",
    "                Xin = X_batch[i,:]\n",
    "                yTrue = y_batch[i]\n",
    "                y_hat = forwardStep(Xin,W_list)\n",
    "                loss += loss_fn(y_hat,torch.tensor(yTrue,dtype=torch.double))\n",
    "            loss = loss/batchSize\n",
    "            loss.backward()\n",
    "            sys.stdout.flush()\n",
    "            dW_list = []\n",
    "            for j in range(len(W_list)):\n",
    "                dW_list.append(W_list[j].grad.data)\n",
    "            W_list = updateParams(W_list,dW_list,lr)\n",
    "            for j in range(len(W_list)):\n",
    "                W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch=%d: %f\" %(epoch,loss/numBatches))\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)\n",
    "\n",
    "W1 = torch.tensor(np.random.uniform(0,1,(2,inputDim)),requires_grad=True)\n",
    "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True)\n",
    "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True)\n",
    "\n",
    "W_list = []\n",
    "W_list.append(W1)\n",
    "W_list.append(W2)\n",
    "W_list.append(W3)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "#W_list = trainNN_sgd(X,y,W_list,loss_fn,lr=0.0001,nepochs=100)\n",
    "#W_list = trainNN_batch(X,y,W_list,loss_fn,lr=0.0001,nepochs=100)\n",
    "W_list = trainNN_minibatch(X,y,W_list,loss_fn,lr=0.0001,nepochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor(np.random.uniform(0,1,inputDim),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = wSum(X[0,:],W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)\n",
    "\n",
    "W1 = torch.tensor(np.random.uniform(0,1,(2,inputDim)),requires_grad=True)\n",
    "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True)\n",
    "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True)\n",
    "\n",
    "W_list = []\n",
    "W_list.append(W1)\n",
    "W_list.append(W2)\n",
    "W_list.append(W3)\n",
    "z = forwardStep(X[0,:],W_list)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss_fun = nn.BCELoss()\n",
    "lr = 0.0001\n",
    "x = torch.randn(1)\n",
    "y = torch.randint(0,2,(1,),dtype=torch.float)\n",
    "w = torch.randn(1,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nIter = 100\n",
    "for i in range(nIter):\n",
    "    y_hat = m(w*x)\n",
    "    loss = loss_fun(y_hat,y)\n",
    "    loss.backward()\n",
    "    dw = w.grad.data\n",
    "    with torch.no_grad():\n",
    "        w -= lr*dw\n",
    "    w.grad.data.zero_()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset,DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 10\n",
    "n = 1000\n",
    "X = np.random.rand(n,inputDim)\n",
    "y = np.random.randint(0,2,n)\n",
    "\n",
    "tensor_x = torch.Tensor(X)\n",
    "tensor_y = torch.Tensor(y)\n",
    "Xy = TensorDataset(tensor_x,tensor_y)\n",
    "Xy_loader = DataLoader(Xy,batch_size=16,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(inputDim,200),\n",
    "    nn.ReLU(),\n",
    "    #nn.BatchNorm1d(num_features=200),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(200,100),\n",
    "    nn.Tanh(),\n",
    "    #nn.BatchNorm1d(num_features=100),\n",
    "    nn.Linear(100,1),\n",
    "    nn.Sigmoid()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nepochs = 100\n",
    "for epoch in range(nepochs):\n",
    "    for X,y in Xy_loader:\n",
    "        batch_size = X.shape[0]\n",
    "        y_hat = model(X.view(batch_size,-1))\n",
    "        loss = loss_fn(y_hat,y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(float(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    xt = torch.tensor(np.random.rand(1,inputDim))\n",
    "    y2 = model(xt.float())\n",
    "    print(y2.detach().numpy()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
